{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import  opennmt.inputters.record_inputter as inpu\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import time\n",
    "import torch.utils.data as data\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from pytorch_i3d import InceptionI3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class I(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(I, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return 'identity'\n",
    "\n",
    "\n",
    "class VideoClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VideoClass, self).__init__()\n",
    "        load_model = '/home/alptekin/Desktop/pytorch-i3d-master/models/rgb_imagenet.pt'\n",
    "        self.model = InceptionI3d()\n",
    "        self.model.load_state_dict(torch.load(load_model))\n",
    "        self.model.fc = I()\n",
    "        self.fc_out = torch.nn.Linear(in_features=1024, out_features=61)\n",
    "\n",
    "    def get_class(self,x):\n",
    "        x = self.model.extract_features(x)\n",
    "        out = self.fc_out(x.squeeze())\n",
    "        return out \n",
    "\n",
    "    \n",
    "    def get_feature(self,x):\n",
    "        x = self.model.extract_features(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_for_balanced_classes(images, nclasses,power=1,threshold=None, flag = 1):                        \n",
    "    count = [0] * nclasses                                                      \n",
    "    for item in images:                                                         \n",
    "        count[item[1]] += 1\n",
    "    weight_per_class = np.zeros(nclasses)       \n",
    "    N = float(sum(count))                                                   \n",
    "    for i in range(flag,nclasses):\n",
    "        if count[i] != 0:\n",
    "            weight_per_class[i] = N/float(count[i])\n",
    "    weight_per_class = np.array(weight_per_class)**power\n",
    "    if threshold is not None:\n",
    "        weight_per_class /= weight_per_class[weight_per_class!=0].min()\n",
    "        weight_per_class = np.clip(weight_per_class,a_max=threshold,a_min=0)\n",
    "    if flag:\n",
    "        weight_per_class[0] = 0.01\n",
    "    weight = [0] * len(images)\n",
    "    print(weight_per_class)\n",
    "    for idx, val in enumerate(images):                                          \n",
    "        weight[idx] = weight_per_class[val[1]]                                  \n",
    "    return weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceFilelist(data.Dataset):\n",
    "    def __init__(self, image_list, transform=None):\n",
    "        self.imgs = image_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        impath, target = self.imgs[index]\n",
    "        np.random.seed()\n",
    "        indi = np.random.randint(16)-15\n",
    "        video_imgs = torch.zeros(3,16,224,224)\n",
    "        for j,i in enumerate(range(indi,indi+16)):\n",
    "            aux_index = index + i\n",
    "            if aux_index < 0:\n",
    "                aux_index = 0\n",
    "            if aux_index > (len(self.imgs)-1):\n",
    "                aux_index = len(self.imgs)-1\n",
    "                \n",
    "            aux_impath, _ = self.imgs[aux_index]\n",
    "            img = self.img_loader(aux_impath)\n",
    "            \n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            video_imgs[:,j,:,:] = img\n",
    "            \n",
    "        return video_imgs, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def img_loader(self,path):\n",
    "        return Image.open(path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "trans_train = transforms.Compose([\n",
    "    torchvision.transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])\n",
    "])\n",
    "\n",
    "trans_test = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "test_batch_size= batch_size * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "video_list = dict()\n",
    "f = open('danish_nz_images.txt') \n",
    "count = 0\n",
    "counts = [0] * 61\n",
    "for l in f.readlines():\n",
    "    path,label = l.split()\n",
    "    my_path = path.split('/')\n",
    "    video_name = my_path[-2]\n",
    "    my_path = os.path.join(*my_path[-3:])\n",
    "    label = int(float(label))\n",
    "    data_list.append((my_path,label))\n",
    "    if label == 0:\n",
    "            continue\n",
    "    if video_name in  video_list:\n",
    "        video_list[video_name].append((my_path,label))\n",
    "    else:\n",
    "        video_list[video_name] = [(my_path,label)]\n",
    "    counts[label] += 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for key,vd in video_list.items():\n",
    "    data_list.append(vd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data_list)\n",
    "data_size = len(data_list)\n",
    "dev_size = round(data_size * 0.1)\n",
    "dev_list = data_list[:dev_size]\n",
    "train_list = data_list[dev_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list_ = list()\n",
    "for t in train_list:\n",
    "    train_list_.extend(t)\n",
    "train_list = train_list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_list_ = list()\n",
    "for t in dev_list:\n",
    "    dev_list_.extend(t[10:-10])\n",
    "dev_list = dev_list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_dist(l):\n",
    "    count = np.zeros(61)\n",
    "    for a in l:\n",
    "        count[a[1]] += 1\n",
    "    return count[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = get_class_dist(train_list)\n",
    "d_d = get_class_dist(dev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_d/t_d.sum())\n",
    "plt.plot(d_d/d_d.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SequenceFilelist(image_list=train_list, transform=trans_train)\n",
    "dev_data = SequenceFilelist(image_list=dev_list, transform=trans_test)\n",
    "\n",
    "weights = make_weights_for_balanced_classes(train_list, 61,threshold=120,power=0.5,flag=0)  \n",
    "weights = torch.DoubleTensor(np.array(weights))\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=sampler,num_workers=8, \n",
    "                                           worker_init_fn=worker_init_fn)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_data, batch_size=test_batch_size,num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = VideoClass()\n",
    "model.to(device)\n",
    "model.train()\n",
    "print('model created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = torch.nn.CrossEntropyLoss()\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(lr=lr,params=model.parameters(),weight_decay=2e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer,step_size=4,gamma=0.2)\n",
    "alpha = 0.2\n",
    "my_step = 39\n",
    "inner = 4\n",
    "report_number = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(1,40):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_aux_loss = 0\n",
    "    start = time.time()\n",
    "    it = iter(train_loader)\n",
    "    scheduler.step()\n",
    "    pred_list = list()\n",
    "    label_list = list()\n",
    "    #####################\n",
    "    for i in range(iter_num):\n",
    "        #######################\n",
    "        optimizer.zero_grad()\n",
    "        ########################\n",
    "        x,y = it.next()\n",
    "        logits = model.get_class(x.to(device))\n",
    "        loss = crit(logits,y.to(device))\n",
    "        preds = logits.argmax(dim=1)\n",
    "        pred_list.extend(list(preds.detach().cpu().numpy().reshape(-1)))\n",
    "        label_list.extend(list(y.detach().cpu().numpy().reshape(-1)))\n",
    "        ########################\n",
    "        epoch_loss += loss.item()\n",
    "        ########################\n",
    "        total_loss =  loss  \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % report_number == 0:\n",
    "            ac = accuracy_score(y_pred=pred_list, y_true=label_list)\n",
    "            print((i+1),epoch_loss / report_number, ac,end='\\r')\n",
    "            pred_list = list()\n",
    "            label_list = list()\n",
    "            epoch_loss = 0\n",
    "            epoch_aux_loss = 0 \n",
    "        if (i+1) % (iter_num//inner) == 0:\n",
    "            model.eval()\n",
    "            pred_list = list()\n",
    "            label_list = list()\n",
    "            for ti,(x,y) in enumerate(dev_loader):\n",
    "                with torch.no_grad():\n",
    "                    logits = model.get_class(x.to(device))\n",
    "                preds = logits.argmax(dim=1)\n",
    "                print(ti/len(dev_loader),end='\\r')\n",
    "                pred_list.extend(list(preds.cpu().numpy().reshape(-1)))\n",
    "                label_list.extend(list(y.cpu().numpy().reshape(-1)))\n",
    "            print('Test Accuracy:',accuracy_score(y_pred=pred_list, y_true=label_list),scheduler.get_lr())\n",
    "            model_path = 'model' + str(my_step) + '.pth'\n",
    "            torch.save(model,model_path)\n",
    "            model.train()\n",
    "            my_step += 1\n",
    "\n",
    "    print('----------------------'+str(epoch)+'------------------------')\n",
    "    print('------------------------------------------------')\n",
    "    print('Loss:',epoch_loss/iter_num,scheduler.get_lr()[0])\n",
    "    print('Elasped Time:', round(time.time()-start))\n",
    "    start = time.time()\n",
    "    print('------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "pred_list = list()\n",
    "label_list = list()\n",
    "for ti,(x,y) in enumerate(dev_loader):\n",
    "    with torch.no_grad():\n",
    "        logits = model.get_class(x.to(device))\n",
    "    preds = logits.argmax(dim=1)\n",
    "    print(ti/len(dev_loader),end='\\r')\n",
    "    pred_list.extend(list(preds.cpu().numpy().reshape(-1)))\n",
    "    label_list.extend(list(y.cpu().numpy().reshape(-1)))\n",
    "print('Test Accuracy:',accuracy_score(y_pred=pred_list, y_true=label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = np.array(pred_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
